# Codex Recommendations

## Scope
- Reviewed the FastAPI API (`app/`), bridge service (`nina_bridge/`), background workers, and dashboard templates to understand how state flows between services, UI panes, and the NINA hardware facade.
- Cross-referenced the NINA Advanced API description (`documentation/advanced-api-openapi-source.json`) to confirm what work can be delegated back to NINA (e.g., built-in plate solving during captures).
- Recommendations below prioritize robustness & simplification of service interplay rather than new functionality.

## Service & State Interplay
1. **Collapse redundant bridge calls.** Every dashboard poll hits `/api/session/dashboard/status`, which synchronously calls `NinaBridgeService.get_status()` (`app/api/session.py`) and the bridge, which itself fans out to four NINA endpoints per request (`nina_bridge/main.py`). Introduce a background poller inside `nina-bridge` that keeps a cached snapshot (camera/mount/focuser/sequence) and serves it immediately, while refreshing hardware telemetry on a steady cadence with retry/backoff. That keeps UI refreshes fast and prevents thundering-herd traffic against the physical mount. Expose cache age + health to detect stale data.
2. **Stop importing `app.services` inside `nina_bridge`.** The bridge currently opens its own DB sessions (`nina_bridge/main.py`) to fetch weather summaries/equipment profiles. This couples two containers tightly and means the bridge cannot answer status if the API DB is briefly offline. Move weather/equipment hydration to a `/bridge/bootstrap` or `/bridge/telemetry` endpoint hosted by the API so the bridge only deals with hardware and cached site metadata, or merge the bridge into the main FastAPI process if keeping two services isn’t buying isolation.
3. **Turn `SESSION_STATE` into a shared store.** Session data is stored in-process (FastAPI) but used indirectly by other services (e.g., automation threads, dashboards). Persist the canonical session JSON to Postgres so the bridge, automation, and UI all read/write through one source of truth instead of relying on Python globals. Use DB row locks or Redis for coordination so concurrent HTMX actions don’t race with background automation.
4. **Let one service own migrations.** All containers built from the root `Dockerfile` run `alembic upgrade head` via `docker-entrypoint.sh` before starting, so on a fresh database multiple instances race and crash when the second one creates an already-existing table. Either (a) set `RUN_MIGRATIONS=0` in `docker-compose.yml` for every service except `api`, letting the API apply migrations once, or (b) introduce a dedicated migration image/startup script that runs Alembic and exits, with the rest of the stack depending on it. Separate Dockerfiles/entrypoints per service are fine if that makes the startup contract clearer.
5. **Standardize container logging.** Right now each service just prints to stdout/stderr, making it difficult to see correlated events when multiple services restart (especially during those Alembic races). Move toward a consistent logging baseline: configure Python’s logging to emit JSON with timestamps + service name, pipe everything through Docker logging drivers or Promtail already in the stack, and ensure every container logs its health checks and retry cycles. Capture this in a shared base image or entrypoint so future services inherit the same logging stack “for free.”
6. **Keep device control opt-in.** Several components (AutomationService, dashboard HTMX actions) call `bridge.connect_telescope(True)` or similar as part of “start session.” That’s risky if NINA is already driving hardware or if the operator wants to stage equipment manually. Update automation to only issue connect/slew/capture commands when the operator explicitly clicks “Connect” in the Overview pane; otherwise, limit the bridge to reporting `Connected` vs `Disconnected` status. This reduces surprise reconnections and keeps the dashboard purely informational unless the user asks for control.
7. **Emit explicit events instead of filesystem polling.** Automation currently waits for FITS files by repeatedly globbing `/data/fits` (`app/services/automation.py`). Replace this with a capture-complete hook driven by the bridge’s existing `camera/capture` proxy (`nina_bridge/main.py:409-479`):
   - When automation wants N exposures, call a new bridge endpoint (or keep using `/camera/capture`) that sets `stream=true` and `waitForResult=true` so the NINA API streams the image back to us (`documentation/advanced-api-openapi-source.json` for `/equipment/camera/capture`).
   - The bridge already writes the streamed bytes to `/data/images/<timestamp>.fits`; extend it to emit a small JSON payload (over HTTP webhook or Redis pub/sub) containing the final filesystem path, target name, and (optionally) the `PlateSolveResult` that NINA can include when `solve=true`.
   - Update `AutomationService._monitor_captures()` to subscribe to those events instead of crawling the shared folder. If an event doesn’t arrive before a timeout, fall back to the existing glob-based search for resilience.
   - Once the API receives the callback it can immediately enqueue astrometry work and record the capture in `CaptureLog`, eliminating the current delay + race window.
5. **Sequence vs. exposure divergence.** Documentation says we pivoted to “individual exposures,” but `AutomationService.run_plan()` still pushes a NINA sequence (`/sequence/start`). Clarify the single path: either keep using advanced sequencer JSON and monitor progress there, or rework automation to call the `camera/capture` endpoint repeatedly through the bridge (which already supports downloads). Mixing both modes increases race conditions when the UI tries to pause or stop automation.

## Dashboard & UI Pane Robustness
1. **Reduce per-pane recomputation.** Most partials (status, camera list, targets, solutions) each perform fresh DB queries and bridge calls (`app/dashboard.py`). Cache the result of `session_dashboard_status()` and share it across panes rendered during the same HTMX request cycle, or switch to SSE/WebSocket updates. This shortens the critical path for the Overview panel and prevents telemetry panels from diverging during heavy load.
2. **Surface blockers consistently.** `_render_status_panel()` hand-filters some blocker reasons coming back from the bridge, but the raw list is still exposed via `/api/session/dashboard-status`. Normalize blocker types (weather/manual override/dome/camera-state) on the bridge side so both the API and UI present the same readiness state.
3. **Provide clear dependency health in UI.** Add a lightweight health summary to the Overview tab (cache age of observability, fetcher status, astrometry worker reachability). Operators currently have to infer problems from log spam because the UI only shows the bridge status.

## NINA Plate Solving Opportunities
1. **Use built-in solving for centering and metadata.** The NINA Advanced API exposes `/equipment/camera/capture?solve=true` and returns a `PlateSolveResult` with RA/Dec, pixel scale, rotation, and success flags alongside the image payload (`documentation/advanced-api-openapi-source.json`). Extend `nina_bridge.camera_capture()` to accept a `solve` flag, pass it through, and return the solve payload even when `omitImage=true`. The API can store this alongside the capture log, giving instant WCS hints.
2. **Tiered solving strategy.** Use the NINA solve result as the “fast path” (for centering + quick QC). If a solve succeeds and the uncertainty is within thresholds, skip the astrometry-worker run and persist the metadata directly. Otherwise, fall back to the current astrometry.net container. This reduces load on the local solver while keeping a verified backup when needed.
3. **Expose solving through automation presets.** Allow presets to declare whether they require NINA-side solving, local solving, or both (e.g., per target magnitude). Automation can then request `solve=true` for the first frame of every series to confirm pointing without blocking the rest of the capture train.

## Suggested Next Steps
1. Prototype the bridge-side telemetry cache + health endpoint; update the API/dashboard to consume cached status instead of calling NINA every refresh.
2. Decide whether to fold the bridge into the API container or to formalize a tiny gRPC/HTTP contract so it no longer imports DB/session code from `app/`.
3. Add a webhook-style callback from `nina-bridge` to the API for capture completion (include file path + optional NINA solve result) and retire the filesystem polling thread once that’s reliable.
4. Wire a basic settings flag to enable `solve=true` requests through the bridge, capture the returned `PlateSolveResult`, and measure how often it can replace local astrometry runs.
